# MutliBandits


Multi-armed bandits (MABs) are a simple but powerful framework for sequential decision making under uncertainty.

This project aims to build four powerful Multi-armed bandits (MABs) algorithms from scratch. 

The following algorithms are included: 
1.  Epsilon greedy (Îµ-greedy) MAB
2.  UCB MAB 
2.  Off-policy evaluation 
3.  LinUCB contextual MAB 
4.  TreeBootstrap contextual MAB  
6.  KernelUCB contextual MAB 
7. Evaluation and hyperparameter tuning for LinUCB and KernelUCB
